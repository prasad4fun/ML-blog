<h1>Defn</h1>
ML is one of the applications of artificial intelligence (AI), that iteratively learn from the data, unlike typical programming.
ML algos can find insights in the data even if they arenâ€™t specifically instructed what to look for in the data.
Terms
Classification: When we deal with categorical data, it is termed as classification. ex: Given person's Height and weight predict the gender. (predicting the class male vs female, Male and female are categories henceforth called categorical data.)
Regression: When we deal with continuous data, it is termed as Regression.          ex: Given house size and no of floors predict the House selling price.(Predicting a continuous number of 52.5 lakh, 52.5 is a continuous data unlike)
Branches
Supervised Learning: Uses Labeled data for prediction.(Eg: previous house sales with info how much each house sold for -- labeled dataset.), Once your model is ready we can use it for prediction where we only need features(size and floors of the house) of the unseen data to be given to the model.

Unsupervised Learning: Unlabeled data, where we can cluster data into similar groups by some common patterns among them. It's then up to the data-scientist how to interpret the clusters.

Reinforcement learning: It works on trial and error methodology, example use cases are like a computer playing video games...

                                       Math Essentials
scalar vs vector:  Vector is something which got magnitude and direction, and a scalar helps to scale up the data.



ex: let's say a man walks at 1kmph(magnitude) in North East direction, which can be represented as [1, 1.5], now if we scale up the same vector by multiplying by 2 (magnitude/scalar), he reaches a point at [2, 3] after 2 hours.

A vector can be more than two dimensional, our computers are good at calculating Dot products of vector and hence in the following tutorial, we try to see a vectorized implementation for all mathematical equations.

Line Equation:  y = mx + b,  aline equation which fits a linear relationship between x & y.

                             Simple Linear regression


Let's say we have to fit a straight line to the given data, as shown above, the line equation can be as follows.

slope m = rise/run, (14-12)/(2-1) = 2

Y-intercept b = 9 (where line crosses y-axis, if we imagine X-axis starting from zero, Line would be crossing y-axis at y=9)

Y = 2X + 9

By this equation, we can predict any unseen point y on the graph given x, which let's say x=11, then we can predict Y = 31 by using the line equation.

Now if our points are little scattered unlike the previous example, we try to fit a better line of approximation which can do a decent job for all the given points.



Model:  Now based on above line equation we can approximate any seen or unseen y given x. This is something which we refer to the linear regression Model.

Problem: Now the problem is to figure out a such a Straight line like in the above figure which can approximate by considering all the given points.

We have to make use of Loss function and gradient Descent to achieve the same.

                                   Cost/Loss Function
Cost function tells us how well our model fits into the data.

If we rewrite the line equation y =mx + b as a function, f(x) = b + mx

Hypothesis function

y` = h(x) = T0 + T1x

theta = weights, x = features, y = output, y` = expected output by hypothesis function

Mean Squared Error: Cost Function helps us to measure the accuracy of the Hypothesis function, By taking average difference between the y` and y

J(T0, T1) = 1/2m sum(y`-y)**2

Now we have to automate finding T0 and T1 which minimizes the cost function J, so that we get the best model to work with.

To automatically find T0 and T1, we can use

Gradient descent:

We keep changing T0 and T1 to reduce J(T0, T1)

Intution:

eqn

lets try to minimize only one parameter T1 for simplicity
$${
J(\theta) =\frac{1}{2m}
[\sum^m_{i=1}(h_\theta(x^{(i)}) -
y^{(i)})2 + \lambda\sum^n_{j=1}\theta^2_j
}$$
